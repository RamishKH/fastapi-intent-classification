{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898e804-484f-4aa3-b722-b76eb98a79c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.5.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/.venv/lib/python3.12/site-packages (from transformers) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/.venv/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /root/.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.11.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: psutil in /root/.venv/lib/python3.12/site-packages (from accelerate) (6.1.1)\n",
      "Collecting torch>=2.0.0 (from accelerate)\n",
      "  Using cached torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Using cached propcache-0.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/.venv/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/.venv/lib/python3.12/site-packages (from requests->transformers) (2024.12.14)\n",
      "Collecting networkx (from torch>=2.0.0->accelerate)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /root/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.5)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch>=2.0.0->accelerate)\n",
      "  Using cached triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: setuptools in /root/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (75.8.0)\n",
      "Collecting sympy==1.13.1 (from torch>=2.0.0->accelerate)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /root/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/.venv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Using cached transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "Using cached datasets-3.4.1-py3-none-any.whl (487 kB)\n",
      "Using cached evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Using cached accelerate-1.5.2-py3-none-any.whl (345 kB)\n",
      "Using cached scikit_learn-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Using cached aiohttp-3.11.14-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "Using cached huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Using cached scipy-1.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.3 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl (766.6 MB)\n",
      "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Downloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "\u001b[2K   \u001b[91mâ”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.8/253.2 MB\u001b[0m \u001b[31m270.9 kB/s\u001b[0m eta \u001b[36m0:15:03\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate accelerate scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b095a188-f169-4c79-821e-90171c0bb46a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())  \u001b[38;5;66;03m# Should print True if a GPU is available\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count())  \u001b[38;5;66;03m# Number of GPUs available\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# GPU model name\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\cuda\\__init__.py:491\u001b[0m, in \u001b[0;36mget_device_name\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    480\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the name of a device.\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \n\u001b[0;32m    482\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;124;03m        str: the name of the device\u001b[39;00m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\cuda\\__init__.py:523\u001b[0m, in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_device_properties\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[0;32m    512\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[0;32m    513\u001b[0m \n\u001b[0;32m    514\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 523\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[0;32m    524\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\cuda\\__init__.py:310\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())  # Should print True if a GPU is available\n",
    "print(torch.cuda.device_count())  # Number of GPUs available\n",
    "print(torch.cuda.get_device_name(0))  # GPU model name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "647117cd-e102-4462-829c-4ee6a2e7801e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krami\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'intent', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 15250\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'intent', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'intent', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"clinc_oos\", \"plus\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets\n",
    "# Convert dataset to PyTorch format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58ed0df5-57a8-49b6-a404-2ff06ca7c33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\krami\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./intent_classifier\\\\tokenizer_config.json',\n",
       " './intent_classifier\\\\special_tokens_map.json',\n",
       " './intent_classifier\\\\vocab.txt',\n",
       " './intent_classifier\\\\added_tokens.json',\n",
       " './intent_classifier\\\\tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.rename_column(\"intent\", \"labels\")\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Split dataset\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "# Load pre-trained model\n",
    "num_labels = len(dataset[\"train\"].features[\"intent\"].names)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "# trainer.train()\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(\"./intent_classifier\")\n",
    "tokenizer.save_pretrained(\"./intent_classifier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c26ce24c-df4c-4815-a6ab-2768c3071da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=151, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model_path = \"./intent_classifier\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e832ef9-9fc9-46f8-b63f-5c685d170dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_intent(text):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Move inputs to GPU if available\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get predicted class\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    return predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe5ba7a9-21cb-4d3b-972f-90814118a18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Intent Label: 138\n"
     ]
    }
   ],
   "source": [
    "text_input = \"What is the weather like today?\"\n",
    "predicted_label = predict_intent(text_input)\n",
    "print(f\"Predicted Intent Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2ca1801-c171-40b7-93c9-9a8733025760",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlabel_name\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'label_name' is not defined"
     ]
    }
   ],
   "source": [
    "label_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a243217-3238-438e-b922-d1d0eeb9565c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 139,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 128,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 97,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " 28,\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['intent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7928f13-63d8-418e-9ad4-67e14a8d8cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
